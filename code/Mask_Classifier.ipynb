{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import random as r\n",
    "import wandb\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seed the behavior of the environment variable\n",
    "os.environ['PYTHONHASHSEED'] = str(1)\n",
    "# Seed numpy's instance in case you are using numpy's random number generator, shuffling operations, ...\n",
    "np.random.seed(1)\n",
    "# Seed Python's random number generator, in case you are using Python's random number generator, shuffling operations, ...\n",
    "r.seed(1)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "set_seed"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.set_seed(seed)>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "BASE_MASK_PATH = '/opt/ml/Haewon/input/data/'\n",
    "TRAIN_MASK_IMAGE_PATH = os.path.join(BASE_MASK_PATH, 'train/crop_images/')\n",
    "TRAIN_MASK_LABEL_PATH = os.path.join(BASE_MASK_PATH, 'train/train.csv')\n",
    "TEST_MASK_IMAGE_PATH = os.path.join(BASE_MASK_PATH, 'test/images/')\n",
    "TEST_MASK_LABEL_PATH = os.path.join(BASE_MASK_PATH, 'test/info.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_data = pd.read_csv('/opt/ml/Haewon/input/data/train/train.csv')\n",
    "train_pre_data = pd.read_csv('/opt/ml/Haewon/input/data/train/train_pre.csv')\n",
    "train_pre_data['gender'] = train_pre_data['gender'].map({'male':0, 'female':1})\n",
    "train_pre_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       id  gender   race  age                                       path  \\\n",
       "0  000001       1  Asian   45          000001_female_Asian_45/normal.jpg   \n",
       "1  000001       1  Asian   45  000001_female_Asian_45/incorrect_mask.jpg   \n",
       "2  000001       1  Asian   45           000001_female_Asian_45/mask1.jpg   \n",
       "3  000001       1  Asian   45           000001_female_Asian_45/mask2.jpg   \n",
       "4  000001       1  Asian   45           000001_female_Asian_45/mask3.jpg   \n",
       "\n",
       "   class  \n",
       "0     16  \n",
       "1     10  \n",
       "2      4  \n",
       "3      4  \n",
       "4      4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45/normal.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45/incorrect_mask.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45/mask1.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45/mask2.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45/mask3.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "img_path = train_pre_data['path']\n",
    "labels = train_pre_data['class']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transform):\n",
    "        self.img_paths = TRAIN_MASK_IMAGE_PATH + img_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = Image.open(self.img_paths[idx])\n",
    "        y = self.labels[idx]\n",
    "  \n",
    "        if self.transform:\n",
    "            x = self.transform(image=np.array(x))['image']\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x = Image.open( TRAIN_MASK_IMAGE_PATH + img_path[0])\n",
    "np.array(x).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(238, 186, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(img_path, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=30)\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "x_valid = x_valid.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def get_transforms(need=('train', 'val'), img_size=(128,128), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            # CenterCrop(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            CenterCrop(img_size[0], img_size[1], p=1.0),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "transform = get_transforms()\n",
    "\n",
    "mask_train_dataset = TrainDataset(x_train, y_train, transform['train'])\n",
    "mask_valid_dataset = TrainDataset(x_valid, y_valid, transform['val'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "mask_train_dataset[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.1446, -0.1943, -0.1777,  ...,  0.0870,  0.0870,  0.0870],\n",
       "         [-0.1446, -0.1777, -0.1612,  ...,  0.0705,  0.0870,  0.0870],\n",
       "         [-0.1612, -0.1777, -0.1777,  ...,  0.0705,  0.0705,  0.0870],\n",
       "         ...,\n",
       "         [-0.1777, -0.1612, -0.1446,  ..., -0.1115, -0.0784, -0.0950],\n",
       "         [-0.1446, -0.1446, -0.1446,  ..., -0.0950, -0.0950, -0.1115],\n",
       "         [-0.1446, -0.1446, -0.1446,  ..., -0.0950, -0.0950, -0.1115]],\n",
       "\n",
       "        [[-0.0083, -0.0241, -0.0083,  ...,  0.2458,  0.2458,  0.2616],\n",
       "         [-0.0083, -0.0083,  0.0076,  ...,  0.2299,  0.2458,  0.2458],\n",
       "         [-0.0083, -0.0083, -0.0083,  ...,  0.2299,  0.2299,  0.2458],\n",
       "         ...,\n",
       "         [-0.0241, -0.0241, -0.0083,  ...,  0.0553,  0.0870,  0.0711],\n",
       "         [-0.0083, -0.0083, -0.0083,  ...,  0.0711,  0.0711,  0.0553],\n",
       "         [-0.0083, -0.0083, -0.0083,  ...,  0.0711,  0.0711,  0.0553]],\n",
       "\n",
       "        [[ 0.0296,  0.0296,  0.0455,  ...,  0.2846,  0.3006,  0.3006],\n",
       "         [ 0.0455,  0.0455,  0.0615,  ...,  0.2687,  0.2846,  0.2846],\n",
       "         [ 0.0296,  0.0455,  0.0455,  ...,  0.2687,  0.2687,  0.2846],\n",
       "         ...,\n",
       "         [-0.0023,  0.0136,  0.0296,  ...,  0.0933,  0.1093,  0.1093],\n",
       "         [ 0.0296,  0.0296,  0.0296,  ...,  0.1093,  0.1093,  0.0933],\n",
       "         [ 0.0296,  0.0296,  0.0296,  ...,  0.1093,  0.1093,  0.0933]]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from multiprocessing import cpu_count\n",
    "# num_workers = int(cpu_count() / 2)\n",
    "num_workers = 1\n",
    "batch_size = 32\n",
    "\n",
    "mask_train_dataloader = DataLoader(\n",
    "    mask_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    "    )\n",
    "\n",
    "mask_valid_dataloader = DataLoader(\n",
    "    mask_valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "images, labels = next(iter(mask_train_dataloader))\n",
    "labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 1, 12,  4,  0,  9,  1,  0,  3,  1,  4,  1, 13, 16, 10,  0,  0,  4,  4,\n",
       "        12,  4, 13,  4,  0,  3,  0, 10,  0,  4,  2,  4, 15, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "efficient = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "for param in efficient.parameters():\n",
    "    param.requires_grad = False\n",
    "efficient._fc = torch.nn.Linear(in_features=1792, out_features=18, bias=True)\n",
    "\n",
    "# torch.nn.init.xavier_uniform_(efficient._fc.weight)\n",
    "# stdv = 1. / math.sqrt(efficient._fc.weight.size(1))\n",
    "# efficient._fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "model = efficient"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 학습 때 GPU 사용여부 결정. Colab에서는 \"런타임\"->\"런타임 유형 변경\"에서 \"GPU\"를 선택할 수 있음\n",
    "\n",
    "print(f\"{device} is using!\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습 때 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 20 # 학습 때 mnist train 데이터 셋을 얼마나 많이 학습할지 결정하는 옵션. 여러 값을 사용해보세요1\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : mask_train_dataloader,\n",
    "    \"valid\" : mask_valid_dataloader\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0 is using!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import wandb\n",
    "wandb.init(project='mask_contest', entity='sala0320', config={\"epochs\" : NUM_EPOCH, \"batch_size\": batch_size, \"lr\" : LEARNING_RATE})\n",
    "wandb.run.name = \"Basic Test Efficientnet-b4\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = '88ec5255a3b1e91fb1eb92834a31072727d5ada2'\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def rand_bbox(size, lam): # size : [Batch_size, Channel, Width, Height]\n",
    "    W = size[2] \n",
    "    H = size[3] \n",
    "    cut_rat = np.sqrt(1. - lam)  # 패치 크기 비율\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)  \n",
    "\n",
    "   \t# 패치의 중앙 좌표 값 cx, cy\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\t\t\n",
    "    # 패치 모서리 좌표 값 \n",
    "    bbx1 = 0\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = W\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "   \n",
    "    return bbx1, bby1, bbx2, bby2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 학습 코드 시작\n",
    "set_seed(42)\n",
    "best_test_accuracy = 0.\n",
    "best_test_loss = 100.\n",
    "early_stopping = 0.\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "  for phase in [\"train\", \"valid\"]:\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    gt = []\n",
    "    pred = []\n",
    "\n",
    "    if phase == \"train\":\n",
    "      model.train() \n",
    "     \n",
    "    elif phase == \"valid\":\n",
    "      model.eval() \n",
    "  \n",
    "    for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      optimizer.zero_grad() \n",
    "\n",
    "      with torch.set_grad_enabled(phase == \"train\"): \n",
    "        logits = model(images)\n",
    "        _, preds = torch.max(logits, 1) \n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        if phase == \"train\":\n",
    "          loss.backward()\n",
    "          optimizer.step() \n",
    "          # scheduler.step() \n",
    "\n",
    "      running_loss += loss.item() * images.size(0) \n",
    "      running_acc += torch.sum(preds == labels.data) \n",
    "      gt.extend(labels.tolist())\n",
    "      pred.extend(preds.tolist())\n",
    "\n",
    "    # 한 epoch이 모두 종료되었을 때,\n",
    "    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "    epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "    epoch_f1 =  f1_score(gt, pred, average='micro')\n",
    "    wandb.log({\"epoch_loss\":  epoch_loss,\n",
    "               \"epoch_acc\":  epoch_acc,\n",
    "               \"epoch_f1\" : epoch_f1})\n",
    "\n",
    "    print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}, F1 : {epoch_f1:.3f}\")\n",
    "\n",
    "    if phase == \"valid\" and best_test_accuracy < epoch_acc:\n",
    "      best_test_accuracy = epoch_acc\n",
    "\n",
    "    if phase == \"valid\" and best_test_loss > epoch_loss:\n",
    "      best_test_loss = epoch_loss\n",
    "      torch.save(model.state_dict(), '/opt/ml/input/data/model/best_model.pth')\n",
    "      early_stopping = 0.\n",
    "\n",
    "    elif phase == \"valid\" and best_test_loss < epoch_loss:\n",
    "      early_stopping += 1\n",
    "      print(\"Early stopping! \" + str(early_stopping))\n",
    "    \n",
    "    if early_stopping > 3:\n",
    "      break\n",
    "\n",
    "    \n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "model._fc = torch.nn.Linear(in_features=1792, out_features=18, bias=True)\n",
    "model.load_state_dict(torch.load('/opt/ml/input/data/train/best_model.pth'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = TRAIN_MASK_IMAGE_PATH + img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = train_pre_data['path']\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(380),\n",
    "    # Resize((380, 380), Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "# model = MyModel(num_classes=18).to(device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_pre_data = train_pre_data\n",
    "test_pre_data['predict'] = all_predictions\n",
    "for i in range(len(test_pre_data)):\n",
    "    if test_pre_data['class'][i] != test_pre_data['predict'][i]:\n",
    "        print(f\"path : {test_pre_data['path'][i]} real : {test_pre_data['class'][i]} predict : {test_pre_data['predict'][i]}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "imgs = submission['ImageID']\n",
    "label = submission['ans']\n",
    "fig, axes = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(12, 6))\n",
    "for i, j in enumerate(range(5)):\n",
    "    axes[i].imshow(Image.open('/opt/ml/input/data/eval/images/' + imgs[j]))\n",
    "    axes[i].set_title(f'{label[j]}')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}